"""
ETL Pipeline V2: Extract from remote view vw_conversations_analytics_final ‚Üí Load to local PostgreSQL
Execu√ß√£o: 1x por dia (agendado para 3h da manh√£)
Estrat√©gia: UPSERT (INSERT ou UPDATE se j√° existir)
"""
import os
import sys
import pandas as pd
import json
from sqlalchemy import create_engine, text
from datetime import datetime
from dotenv import load_dotenv
from urllib.parse import quote_plus

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables
load_dotenv()

# Configura√ß√µes
SOURCE_VIEW = 'vw_conversations_analytics_final'
LOCAL_TABLE = 'conversas_analytics'


def get_source_engine():
    """Cria engine de conex√£o com banco remoto"""
    conn_str = (
        f"postgresql://{os.getenv('SOURCE_DB_USER')}:{os.getenv('SOURCE_DB_PASSWORD')}"
        f"@{os.getenv('SOURCE_DB_HOST')}:{os.getenv('SOURCE_DB_PORT')}/{os.getenv('SOURCE_DB_NAME')}"
    )
    return create_engine(conn_str)


def get_local_engine():
    """Cria engine de conex√£o com banco local"""
    # URL encode da senha para evitar problemas com caracteres especiais (@, etc)
    password_encoded = quote_plus(os.getenv('LOCAL_DB_PASSWORD'))

    conn_str = (
        f"postgresql://{os.getenv('LOCAL_DB_USER')}:{password_encoded}"
        f"@{os.getenv('LOCAL_DB_HOST')}:{os.getenv('LOCAL_DB_PORT')}/{os.getenv('LOCAL_DB_NAME')}"
    )
    return create_engine(conn_str)


def extract_from_source():
    """
    Extrai dados da view remota vw_conversations_analytics_final
    Retorna: DataFrame com todos os dados
    """
    print("=" * 80)
    print("EXTRACT: Buscando dados da view remota")
    print("=" * 80)
    print(f"View: {SOURCE_VIEW}")
    print(f"Host: {os.getenv('SOURCE_DB_HOST')}")
    print(f"Database: {os.getenv('SOURCE_DB_NAME')}")

    try:
        engine = get_source_engine()

        # Query para extrair todos os dados
        query = f"SELECT * FROM {SOURCE_VIEW}"

        print(f"\nExecutando query...")
        start_time = datetime.now()

        df = pd.read_sql(query, engine)

        elapsed = (datetime.now() - start_time).total_seconds()

        print(f"\n‚úÖ Extra√ß√£o conclu√≠da!")
        print(f"   Linhas extra√≠das: {len(df):,}")
        print(f"   Colunas: {len(df.columns)}")
        print(f"   Tempo: {elapsed:.2f}s")

        # Mostrar primeiras colunas
        print(f"\nüìä Primeiras colunas:")
        for i, col in enumerate(df.columns[:10], 1):
            print(f"   {i}. {col}")
        print(f"   ... e mais {len(df.columns) - 10} colunas")

        engine.dispose()
        return df

    except Exception as e:
        print(f"\n‚ùå Erro ao extrair dados: {e}")
        import traceback
        traceback.print_exc()
        return None


def transform_data(df):
    """
    Transforma os dados para adequar ao schema local
    - Renomeia colunas se necess√°rio
    - Converte tipos de dados
    - Trata valores nulos
    """
    print("\n" + "=" * 80)
    print("TRANSFORM: Preparando dados para inser√ß√£o")
    print("=" * 80)

    if df is None or df.empty:
        print("‚ùå Nenhum dado para transformar")
        return None

    try:
        # Criar c√≥pia para n√£o modificar original
        df_transformed = df.copy()

        # Converter message_compiled para JSON string v√°lido
        if 'message_compiled' in df_transformed.columns:
            # PostgreSQL retorna como objeto Python, converter para JSON string
            df_transformed['message_compiled'] = df_transformed['message_compiled'].apply(
                lambda x: json.dumps(x) if x is not None else None
            )

        # Adicionar campos de controle do ETL
        df_transformed['etl_inserted_at'] = datetime.now()
        df_transformed['etl_updated_at'] = datetime.now()

        # Tratar valores NaN/None em campos num√©ricos
        numeric_cols = df_transformed.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_cols:
            df_transformed[col] = df_transformed[col].where(pd.notna(df_transformed[col]), None)

        # Tratar valores NaN/None em campos de texto
        text_cols = df_transformed.select_dtypes(include=['object']).columns
        for col in text_cols:
            if col != 'message_compiled':  # J√° tratado acima
                df_transformed[col] = df_transformed[col].where(pd.notna(df_transformed[col]), None)

        print(f"‚úÖ Dados transformados com sucesso")
        print(f"   Linhas: {len(df_transformed):,}")
        print(f"   Colunas finais: {len(df_transformed.columns)}")

        return df_transformed

    except Exception as e:
        print(f"‚ùå Erro na transforma√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return None


def load_to_local(df):
    """
    Carrega dados no banco local usando estrat√©gia UPSERT
    - INSERT novos registros
    - UPDATE registros existentes
    """
    print("\n" + "=" * 80)
    print("LOAD: Inserindo/Atualizando dados no banco local")
    print("=" * 80)

    if df is None or df.empty:
        print("‚ùå Nenhum dado para carregar")
        return False

    try:
        engine = get_local_engine()

        print(f"Tabela destino: {LOCAL_TABLE}")
        print(f"Estrat√©gia: UPSERT (INSERT ou UPDATE)")

        # Verificar quantos registros j√° existem
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {LOCAL_TABLE}"))
            count_before = result.scalar()
            print(f"\nRegistros atuais na tabela: {count_before:,}")

        start_time = datetime.now()

        # Usar pandas to_sql com m√©todo UPSERT customizado
        # Por simplicidade, vamos usar TRUNCATE + INSERT para primeira vers√£o
        # Depois podemos otimizar para UPDATE apenas registros modificados

        print(f"\nInserindo {len(df):,} registros...")

        # Estrat√©gia 1: Limpar tabela e inserir tudo (mais simples)
        # Para produ√ß√£o, podemos mudar para UPDATE apenas registros alterados
        with engine.connect() as conn:
            # Truncate (limpar tabela)
            conn.execute(text(f"TRUNCATE TABLE {LOCAL_TABLE}"))
            conn.commit()
            print("‚úì Tabela limpa (TRUNCATE)")

        # Insert em batch
        df.to_sql(
            LOCAL_TABLE,
            engine,
            if_exists='append',
            index=False,
            method='multi',
            chunksize=1000  # Inserir em lotes de 1000 registros
        )

        elapsed = (datetime.now() - start_time).total_seconds()

        # Verificar inser√ß√£o
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {LOCAL_TABLE}"))
            count_after = result.scalar()

        print(f"\n‚úÖ Carga conclu√≠da!")
        print(f"   Registros inseridos: {count_after:,}")
        print(f"   Tempo: {elapsed:.2f}s")
        print(f"   Velocidade: {count_after/elapsed:.0f} registros/segundo")

        engine.dispose()
        return True

    except Exception as e:
        print(f"\n‚ùå Erro ao carregar dados: {e}")
        import traceback
        traceback.print_exc()
        return False


def create_backup(df):
    """
    Cria backup CSV dos dados extra√≠dos
    """
    if df is None or df.empty:
        return

    try:
        # Criar diret√≥rio de backups
        backup_dir = "data/backups"
        os.makedirs(backup_dir, exist_ok=True)

        # Nome do arquivo com timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_file = f"{backup_dir}/conversas_analytics_{timestamp}.csv"

        df.to_csv(csv_file, index=False)

        file_size_mb = os.path.getsize(csv_file) / (1024 * 1024)
        print(f"\nüíæ Backup criado: {csv_file}")
        print(f"   Tamanho: {file_size_mb:.2f} MB")

    except Exception as e:
        print(f"\n‚ö†Ô∏è  Erro ao criar backup: {e}")


def print_statistics(df):
    """
    Imprime estat√≠sticas dos dados extra√≠dos
    """
    if df is None or df.empty:
        return

    print("\n" + "=" * 80)
    print("üìä ESTAT√çSTICAS DOS DADOS")
    print("=" * 80)

    try:
        # Estat√≠sticas gerais
        print(f"\nüìà Resumo:")
        print(f"   Total de conversas: {len(df):,}")

        if 'status_label_pt' in df.columns:
            print(f"\nüìä Por Status:")
            status_counts = df['status_label_pt'].value_counts()
            for status, count in status_counts.items():
                pct = (count / len(df)) * 100
                print(f"   {status}: {count:,} ({pct:.1f}%)")

        if 'conversation_date' in df.columns:
            print(f"\nüìÖ Per√≠odo:")
            print(f"   Data mais antiga: {df['conversation_date'].min()}")
            print(f"   Data mais recente: {df['conversation_date'].max()}")

        if 'has_csat' in df.columns:
            csat_count = df['has_csat'].sum()
            csat_pct = (csat_count / len(df)) * 100
            print(f"\n‚≠ê CSAT:")
            print(f"   Com avalia√ß√£o: {csat_count:,} ({csat_pct:.1f}%)")

        if 'has_human_intervention' in df.columns:
            human_count = df['has_human_intervention'].sum()
            human_pct = (human_count / len(df)) * 100
            print(f"\nü§ñ Atendimento:")
            print(f"   Com interven√ß√£o humana: {human_count:,} ({human_pct:.1f}%)")
            print(f"   Apenas bot: {len(df) - human_count:,} ({100 - human_pct:.1f}%)")

    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao calcular estat√≠sticas: {e}")


def run_etl():
    """
    Executa pipeline ETL completo
    """
    print("\n" + "‚ñà" * 80)
    print("  ETL PIPELINE V2 - AllpFit Analytics")
    print("  Extra√ß√£o: vw_conversations_analytics_final (remoto)")
    print("  Destino: conversas_analytics (local)")
    print(f"  Execu√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("‚ñà" * 80 + "\n")

    start_total = datetime.now()

    # 1. EXTRACT
    df = extract_from_source()

    if df is None:
        print("\n‚ùå ETL abortado - falha na extra√ß√£o")
        return False

    # 2. TRANSFORM
    df_transformed = transform_data(df)

    if df_transformed is None:
        print("\n‚ùå ETL abortado - falha na transforma√ß√£o")
        return False

    # 3. LOAD
    success = load_to_local(df_transformed)

    if not success:
        print("\n‚ùå ETL abortado - falha na carga")
        return False

    # 4. BACKUP
    create_backup(df)

    # 5. ESTAT√çSTICAS
    print_statistics(df)

    # Tempo total
    elapsed_total = (datetime.now() - start_total).total_seconds()

    print("\n" + "‚ñà" * 80)
    print("  ‚úÖ ETL PIPELINE CONCLU√çDO COM SUCESSO!")
    print(f"  Tempo total: {elapsed_total:.2f}s ({elapsed_total/60:.1f} minutos)")
    print("‚ñà" * 80 + "\n")

    return True


if __name__ == "__main__":
    run_etl()
